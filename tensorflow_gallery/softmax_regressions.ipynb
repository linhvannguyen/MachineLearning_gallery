{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "by Linh Van Nguyen (contact@linhvannguyen.com)\n",
    "\n",
    "as a part of the demonstration galeries for <a href=\"https://www.tensorflow.org/\"> tensorflow </a>.\n",
    "\n",
    "------------------------\n",
    "This demo if following the tutorial on <a href=\"https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/beginners/index.html\"> Softmax Regression </a>. More informative tutorials on this subject can be found <a href=\"http://ufldl.stanford.edu/tutorial/\"> at UFLDL tutorial</a>\n",
    "\n",
    "Softmax Regression is the way we convert outcomes of a network model into probabilities. It contains two steps:\n",
    "\n",
    "* accumulating evidences (including bias terms) through the network: says, at $i-$th layer, the accumulated evidence from the input $x$ is\n",
    "$$evidence_i = \\sum_jW_{i,j} x_j +b_i = W_ix+b_i$$\n",
    "* converting into probability\n",
    "\n",
    "$$y=softmax(evidence)$$\n",
    "\n",
    "where $softmax(.)$ is defined as a normalization of the exponential of inputs:\n",
    "\n",
    "$$softmax(x)=normalize(exp(x))$$\n",
    "\n",
    "The normalization is to form a probability distribution of all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import input_data # defined function to load data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MINST data\n",
    "This data contains 3 sets: training set (*mnist.train*) of 55000 samples, validation set (*mnist.validation*) of 5000 samples, and testing set (*mnist.test*) of 10000 samples. In each set, one sample contains an image of size 784 (28x28), a label of size 10 (all zeros except one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(5000, 10)\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAW8AAADTCAYAAABOQ5KuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAGQNJREFUeJzt3Xm8nOP5x/FPiJ2IUFFbbG0Ste+xb0GqlEgU1aSopUqU\n",
       "WBqRaC2lsYVXiK2pClLUnmrUFiLVihatl9CEoLFEUkKjtuD3h9937llzlszyXJPv+58z55k5M/d5\n",
       "zpxrruderrvDl19+iZmZxbJYoxtgZmZt5+BtZhaQg7eZWUAO3mZmATl4m5kF5OBtZhaQg7eZWUAO\n",
       "3mZmATl4m5kF1LEOr7EoLeHs0MrH+ZyU8jkpb1E5Lz4n5VU8L868zcwCcvA2MwvIwdvMLCAHbzOz\n",
       "gOoxYGlmAX3xxRcADB48GIBRo0YB8OSTTwKw1VZbNaZhBjjzNjMLyZm3mRV45513ABg2bBgA1157\n",
       "bcH9M2bMABatzPvoo4/O3b7pppsAmDx5MgBbbLFFQ9rkzNvMLCAHbzOzgNxt0uRee+01AK677rrc\n",
       "sfPPPx+ADh2+WrylfUx79uwJwHnnnQdA375969ZOa7y33noLgBEjRgCl3SU77bQTANtuu219G5YB\n",
       "3bp1y93++OOPAZg2bRrgbhMzM2uDDnXYPd51CErV7JzMnj0bgAsuuACAm2++GYA5c+akF///v3lx\n",
       "5q3v1157bQCmTJkCwCqrrLIwTWrYOfn0008B2GOPPQB44oknCu7v3Llz7vY//vEPANZaa61qN6Oc\n",
       "TNXxmD9/PgA//elPAbjyyisL7v/JT34CwKWXXgrAkksuWYtmZOqcFBs7dmzu9oABAwDo06cPAPff\n",
       "f38tX9q1TczMmknoPu/f/OY3QMoYV155ZQCmTp2ae0yvXr2A1F/XrNRPreldlbJqSJn11772tYLn\n",
       "UHb+6quvArDzzjsD8MILL9So1bWhjPuoo44CSjPuAw44AICf/exnuWOrr756q5571qxZAHTt2nWh\n",
       "25kVQ4YMAUoz7mOPPRZIi3OsUI2uQFrNmbeZWUANy7xvueUWAJ555pncsTFjxrTpOebOnVvwfceO\n",
       "X/06yrwAll56aQCWXXZZADbZZBMAbrvtNqA0+4zqnnvuAVKGnZ9pA2y44Ya52xMnTgRK+7InTZoE\n",
       "wC677ALASy+9VJO21toll1wCpMUUor7biy++GEjvjdbQEnFd7Q0fPhxI/cQRnX322UA6H3LCCScA\n",
       "qY/b4K677io5duihhzagJYkzbzOzgOo+2+SUU04B4PLLLwdS8Zt622233QAYN24cULU+zLrPrFD/\n",
       "/jbbbAOkfn9dUSi7zs+idO7PPPNMIPWBS3H2fvXVVwNwzDHHtKeJdTsnzz//PJDOxUcffQTACius\n",
       "AMC7774LpCu01tCMm3322afgOS677DKg3Zl3Q2dW/OUvfwHg29/+NgDvvfcekPq4r7rqKgAWW6yu\n",
       "uV0mZ5uoZ2C77bbLHevUqRMAr7/+OgDLLLNMLZvg2SZmZs2k7n3et99+O5AybvVBQ8ufYDvssAOQ\n",
       "Zgu0xkMPPQTAjTfeCKSZFI8++iiQ+q1uvfVWIF4fuFZFFs/JLu7Pzl8tp9vKpJV533nnnUBp5h1l\n",
       "peWFF14IpIx7iSWWAODee+8F2pZxi/qDlXFrhkFb3oNZo/56Zdz77bcfkGYq1TnjzjSNn+WPo+n8\n",
       "1DjjbpH/SmZmAdU983744YeB1D/Zu3fv3H3qm6wmze8eOHAgAPvuuy8AL774IpAycGXmmlUQTY8e\n",
       "PRZ4f34m3r17dyD1j6v/VpmrxkGK+82z7m9/+1vB9+qn3nXXXQuOf/7550BhNlXs5ZdfBuCxxx4r\n",
       "OH7QQQcBsM466yxMUxvqn//8Z8H3Kne6xhprNKI5mXbHHXc0ugkVOfM2Mwuo7pn3N7/5zYKv9bLe\n",
       "eusBcO655wLQv3//gvuVdUbNvOXxxx8H0pWFsmb1jUOav63qcCq+rz7uVVddFYA//vGPdWhx7Xzy\n",
       "yScF3z/11FMAnHXWWQA8+OCDrX6u1VZbDUgzdCIaP348AG+//TaQxjK+853vNKxNWadKi1nkzNvM\n",
       "LKDQtU2slFauakZJudomOqaMu7iP+8QTTwQaV6e4vc444wwAjjjiCCCNZ+y+++5A6r9uz9oC9Qtv\n",
       "tNFGC93ORtFsIunXrx9Quhq3tXQePTulMXzWzcwCcvA2Mwtokek20ZLfp59+uuz9Wtih6WZbbrll\n",
       "fRpWI8WXwuUujXVMpV+1hD5ad4loubJ89tlnQOo+ES11PvDAA3PH3njjDQCuuOKKss/dDDula6GR\n",
       "aKpoaz355JNAKpcwc+ZMIC2869Kly8I2MTM0jXTGjBkl97U0LbdenHmbmQUUOvPWNB6V/tRikwU9\n",
       "tpIPP/wQSINb77//fjWaWHeHHXYYkDYe1gYLmjoIMG/evIKfOeecc4C4GbcceeSRQOUi+YcccgiQ\n",
       "tjpbfPHFc/dp27hiO+64I5CKOEWkZfBaINda+p/QVaiy0OLFTSo2d8MNNyxMMzNFv/vkyZNL7ttz\n",
       "zz3r3ZyynHmbmQUUKvNWkSn1S19zzTVA+X6p9lL2FpX6r/VV8jPvoUOHAnD33XcDaWGSFuVEWQ5f\n",
       "bM011wQKtzdrreWWW67s8UGDBgHtK2qVFdpguPiKqxKVSR4xYgTQ8qYcUa9SF2RBV+oqu9BozrzN\n",
       "zALKdDoxbdo0AI477jgAHnnkkQU+vlu3bgCstNJKJfdpWby2vtJWT8VZRWs3oq232bNnA+0vWZs/\n",
       "Qq5iO3369AFgwoQJQBo7iLy1V3sVLzTR9xtssEEjmlNV2gJQBcmK3/MffPABkMoit3XTjUaXRq0F\n",
       "xQvJLyGQlbEhZ95mZgFlMvPWrJFRo0YB8MorrwCw/PLLA7DiiisCcPLJJwMpW95+++2BlIEviJ5D\n",
       "VI42a0V6VGhK/dLKoMeOHbvQz60iSw888AAQd8PhasjfrAJgr732AmDzzTdvRHOqSv35eu/o76zN\n",
       "F1QmQRuVtNZmm20GwMiRI6vRzEwpnpmTfzWfP0upkZx5m5kFlMnMWyu5lHHvv//+QMo+i2dStMWz\n",
       "zz4LpHnQstRSSwGFpVMbRf3bkDaF1QbJ1ci4NYdVz12HTagzKX+WhPp9pRn7/fX3vu+++4BUIre1\n",
       "tCJXRbrUL6wSws1g1qxZQFqdm2XOvM3MAspk5q3aCdqcWMXzq2H69OlA+oSVrKyaArjrrrtyt9U/\n",
       "WbyVV1tNnTo1d1tbeem5lVFlpWZDveRnnroS0+rMZqrTIZpdpExZmzK0RJt0a/Vu1saFqkkzbebO\n",
       "nVtwXL97ljjzNjMLKJOZt7Keambcov506dy5M5BW0mWBNk2G1B+tjQQ0F1t988XVD5VBTpo0CUgF\n",
       "+LWaMv85lXGrf/ekk06q4m+Rfdp0Ip9mNG299db1bk7DaPMKzR456qijgDTXvRnncRdThcTiTax1\n",
       "Rb733nvXvU0tceZtZhZQJjPvWth4442BwhofkObz9urVq+5tqiR/xos2iVXmPGDAACBlzcWrvVTT\n",
       "WtUEy22DJrqyydJVRz0Vb1AMsOmmmzagJY2h2uXHH388kJ35y42gue6q6y4DBw4E2r9VXC058zYz\n",
       "C2iRyby1ekwV1rTCMuvzeTXzRhl18U5A+l6ZQXGmrboW+dn8kCFDgJTVW7IoZJ8t1ba3NO6kNSZZ\n",
       "5MzbzCwgB28zs4A61GFpdEPXXquw/OGHHw6kIj3XX389AAcffHA1X661oxptPicagFQxIdGGFFp4\n",
       "U7yRgqb/NXABTs3OycJad911c7fVraZFOtqwYvjw4bV46baMfi0qtQt8TsqreF6ceZuZBdSUA5b5\n",
       "RWW0lZMyqn79+gFVz7hrThn16NGjC44Xf2+tl79IR0WWtCy6eHMGs6zxO9TMLKCm7PPWdEBIGzto\n",
       "6W/v3r1r+dKZ7d9tIJ+TUu7fLeVzUp77vM3MmklTZt4N5CyzlM9JKWeZpXxOynPmbWbWTOqReZuZ\n",
       "WZU58zYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdv\n",
       "M7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB\n",
       "28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy\n",
       "8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OA\n",
       "HLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28ws\n",
       "IAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAHLzNzAJy8DYzC8jB28wsIAdvM7OAOtbhNb6sw2tk\n",
       "RYdWPs7npJTPSXmLynnxOSmv4nlx5m1mFpCDt5lZQA7eZmYBOXibmQXk4G1mFpCDt5lZQPWYKmgZ\n",
       "c8455wDwu9/9DoDx48cDsN566zWsTfX0wgsvADBy5Mjcseuuuw6AY489FoCrr766/g2zzHjnnXcA\n",
       "eO655wC45557cvc9/vjjADz//PMAHHHEEQCsv/76AAwePBiApZZaquA53333XQC6dOlSlTY68zYz\n",
       "C8jB28wsoKbvNnniiScAuOaaawC46aabyj5up512AqBv374ADBgwAKjeJU6j/ec//8ndVhfBzJkz\n",
       "Afj73/8ONH+3yW9/+1sAhg0bBqTfH6BDh68Wst1///1lf1bvm+9+97sArLDCCjVrpzXO9ddfD8Av\n",
       "f/lLAF577bWSx3z55VcLPPWeueGGGwruX2aZZQA4+eSTC44feuihADzwwANVaaszbzOzgJoq854/\n",
       "fz4AP//5z3PHrrzySgDef/99IH1aFps0aRKQMvVnn30WKP1UjUpZJxRmnM3ss88+A1Kmc8wxxxQc\n",
       "b43Ro0cDMGjQIADWXXddAM4991wAvve971WnsQ308ssvA2kAd/LkyQBMnToVSIO3AwcObEDr6kMZ\n",
       "dqWMW9k0wPLLLw+kWDJnzhwAvvjiCwBOPfVUAFZccUUAjjzySADefPPNqrbZmbeZWUBNlXkPHToU\n",
       "gIsuuih3rLh/qtjOO+8MwGOPPVZw/E9/+hMA//3vf3PHIvdzTpw4sdFNqLtLL70UgCFDhrT42B49\n",
       "egBw0kknFRxXVvX5558DMH36dACOO+64gsdFysB15XHrrbcCKaNecsklgfR/9PTTTwOLRuatmKGM\n",
       "W+eif//+QGH/9eabb17ws7fddhsAF154IZCmF3788ccFj1t99dWr2mZn3mZmAYXOvNXHrUxBmVa+\n",
       "5ZZbDoBTTjkFgAMPPBCAtddeG4BOnToBqV/q5ptvBmCVVVYBoGPH0Kco14evfsxFgTJLZUCVrLXW\n",
       "Wrnb1157LQA77rhjq15DYyha1KMsFQqv/LLk008/BdJsmxEjRgDwrW99C4DLLrsMgN69ewNpbOTf\n",
       "//43kMaF1P+71VZb1aPZdTFu3LiC7/U+uPHGG1v82YMPPhiAVVddFYA99tij7OM0U6lanHmbmQUU\n",
       "Oq1Ullyc6XTv3j13W/1RG2+88QKfS31cssEGGwCFo8wRaUmuvjYz9Uvr/aDl/8U0znHHHXfkjq28\n",
       "8splH7vvvvsCMGPGDADGjh1b8FoffPABkLLXLPrkk08A+NGPfgSkOev6n9CMqi222KLg59Zcc00g\n",
       "jfXod+zZsycADz74YA1bXV/6/9DYWHv+nt/4xjcA6Nq1KwAbbbRRwf2ajVItzrzNzAIKnXlrdFcz\n",
       "SjbbbDMAJkyYkHuMPgWL/e9//wPSiLv6htXXfeedd9agxdmy2mqrASnDim7KlCkAnHXWWWXv3377\n",
       "7QG47777gNbNHlI2OmbMGCDNSlImnlXKtgHOPvtsIGXcm2yyCZDmv+t9UMntt98OwBtvvAGkq9QP\n",
       "P/wQSONKkWksTAWoFBfyi5dVovGO008/HYB58+YBcP755wPpSm+xxaqbKzvzNjMLKHTmLeqnUiZe\n",
       "LttWf5NWTh5++OEAvPjii0DK3tXH2Sw0g6AcZWDbbbddvZpTE+qHVqZTTBn3ww8/DJSW6mxGuroA\n",
       "+NWvfgWkGVa6Mm0p45a5c+cWfN+5c2egOTJuUYb9r3/9C4CXXnoJSGsE8ud5qySs3m9aoaorEXn0\n",
       "0UcB+POf/wzARx99VNU2O/M2MwuoKTJv0TzLcpRxV5qbus8++wCVZyhEpY0HyjnggAPq2JLqU8Zz\n",
       "5plnAqlPVtTXqCx0YTLuadOmAaXZlepXZKUio6pHnnbaabljqsWhlZJf//rXW/Vcb731FgC///3v\n",
       "q9nETNJVicYHDjnkECDNhddXaHnV9jbbbAPA3nvvDaRZKFoToM0aFpYzbzOzgEJn3up7E2Vam266\n",
       "ae6YPvWKswdlYSeeeCKQtgZbeumla9PYDIrev3/QQQcBpRm3qH5yNWrSKGvV9liyxhprAOm912ha\n",
       "+fnqq6/mjqkWR58+fRb4s5q7rnnfqrD3yiuvVLmV2aN+7HKrtFuyyy67ADBq1CggbYdW67EVZ95m\n",
       "ZgE5eJuZBRS62+TXv/41kJahajBJU3MgFWQqHly44oorADj66KNr3s5G0PQ5XUbn0wDW4osvXtc2\n",
       "VYtKHmiap2jqWq9evYDqdAu9/fbbQCpcVazaZT5rQYWltNimuOTDvffeC6TzqvfMOuusA8AZZ5wB\n",
       "pCmHrZ1iGMHdd98NwPDhw4G0I/yCaMBSMeSEE05o1Wvp56rFmbeZWUAhM28tZb/llluA1n2i6TGa\n",
       "HtesGbcWVOiqJH+ZtGjBgQbbotFgnEqciq7AtJFGNWiz5uIpghqMUlaaFdqmTVPeAH7xi18AqXRp\n",
       "JSqRqy3etOGEMndl3lr0FJkGnrX5hn5HXaHr77v//vsDhZsG68pk2WWXbdNrVppa2F7OvM3MAgqR\n",
       "eWuqkjZMUHEgfZIVf6JpkjzArrvuCqTysY888giQylmq8HyzUOZdvK1b/rQlTWVqNtUsdq8rNU2f\n",
       "K6aSApUK7zeK/hfyN+HecMMNgdS/K+q7VkZeqUyCFiCp8JtK6VYqAJZlyrA1nVhZtKaT6ndSrFGp\n",
       "4OOPPz73HJo2qsVfP/zhD4GWC0/9+Mc/Xuj253PmbWYWUKYzb42ODxgwACjffwuw7bbbAml2Qf6n\n",
       "ZJcuXYCUXWh5vPq6FrR8PKJKxW9WWmml3O1m3Uh2hx12qNpz/eEPfwDS4q1iu+++e9Veq9b03m+p\n",
       "z7sSbcKtDQsqbVwRwXnnnQekjFvjPpo5UqlkxFVXXZW7rXLAmqWjsTcVu6skPy5VgzNvM7OAMpl5\n",
       "a2S3OOPWcniVMlW5xt122w0o3cosn/rrNJ9TS3+feuopoLCfPDJdURRTkZxmpr+tSnG2xZw5c4A0\n",
       "Syd/tkY+jRf84Ac/aE8TQ5o9ezYAr7/+OpA2LohImy2IsubWbjwNaWxFs5pUGralzLvanHmbmQWU\n",
       "ycz7ueeeA1LG3a1bNyDNENHmwG2hOcF//etfAZg/f37B1+iUHb333nsFx9U3q6I5zUwlTFWoqtI8\n",
       "dmWQmoEEMHr0aABmzpy5wNcYN24ckFYfLgomTpxY8L22CoxIs4j0VWNibaGxA210ojnj2oy6U6dO\n",
       "C93O1nDmbWYWUCYzb9GnY79+/YD2Zdz6NNRzKHtvNurn1WaoolVgHTumP7WuNvKPRaL+Zm0S+8wz\n",
       "zwBpCytdbVTKqrRhwfTp01t8LV31qTi/VnEuSjTLpBlozEJjHJdccgmQxs9aE2NUE0hjbFpboT5w\n",
       "xZpiGsur1viTM28zs4AymXpp9ZM2Rijurx06dChQuhmDMiptHgpw2GGHAamfUyvQtOpMheqb1fjx\n",
       "44HCSnLDhg0DKs9hzjpt46XVcPoba4xEW5a1xxJLLAFAz549gZTdd+/evd3PadmhmSIa+9LGE1pT\n",
       "oiurBWXHl19+OZDmimsMYL/99lvga5966qktPndbOPM2Mwsok5m3PpkuuugiAAYNGgSk/qkxY8YA\n",
       "pVtPTZgwAShciVm8WahWY6paXHFt46jUv6sNcYvreCujhLjVBItpvvGWW24JpNWy6oNsC12JaX53\n",
       "//79q9HEpqR1FhFpE+CRI0cCaabIvHnzgJSR62s5xTFF/08tbXu29dZbt7fZZTnzNjMLKJOZt6jf\n",
       "sUePHkDKqDSft3i1VDn62e9///sAnH766cCCV2NGtOeeewJpfEAzMrSydPDgwbnH1nslWK2pvvub\n",
       "b74JpFVzqn6nLOqCCy4Ayu8gpExbs0usMm3qHZHGyaZMmQKkK3BVXGzNTjracFj954otLVGPQbU4\n",
       "8zYzC8jB28wsoA7V3hSzjKq9wKxZs4DSIvAPPfQQAF27dgWgb9++ufvUTVInrd3nqOYnPUN8Tkq1\n",
       "ZT+shp+Xiy++GIDTTjsNSAPD6tasklDnpI4qnhdn3mZmAWV6wLKYMmsNMphZ/ajgkrYMs8Zy5m1m\n",
       "FlCoPu8A3L9byueklPt3S/mclOc+bzOzZuLgbWYWkIO3mVlA9ejzNjOzKnPmbWYWkIO3mVlADt5m\n",
       "ZgE5eJuZBeTgbWYWkIO3mVlADt5mZgE5eJuZBeTgbWYWkIO3mVlADt5mZgE5eJuZBeTgbWYWkIO3\n",
       "mVlADt5mZgE5eJuZBeTgbWYWkIO3mVlADt5mZgE5eJuZBfR/Wqfqk0AzE/IAAAAASUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73da9b3d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "print np.shape(mnist.train.images)\n",
    "print np.shape(mnist.validation.labels)\n",
    "print mnist.test.num_examples\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.reshape(mnist.train.images[i],(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple network for regression\n",
    "\n",
    "TensorFlow is built such that a graph of interacting operations is described and run entirely outside Python.Symbolic variables are used to describe such interacting operations. In this example, let define a symbolic variable, a 2-D tensor of floating-point numbers with shape $[None, 784]$ ($None$ means any dimension), to handle with the input image (in colum-liked form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle model's parameters (weights and biases), TensorFlow uses *Variable*, a modifiable tensor that lives in TensorFlow's graph of interacting operations. Here the network is defined to contain only one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizes of the parameters should be consistent such that outputs of the network\n",
    "\n",
    "$$y_i = softmax(Wx_i+b)$$\n",
    "\n",
    "The model is then defined in tensorflow as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the matrix multiplication is done by *tf.matmul*, where $x$ and $W$ are flipped to handle multiple images at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Define the cost function as the *cross-entropy*:\n",
    "\n",
    "$$H_{y'}(y)=-\\sum_i y_i' log(y_i)$$\n",
    "\n",
    "where $y$ is the predicted probability distribution, and $y_i'$ is the true one. As an intepretation, the *cross-entropy* function measures how inefficient the predictions describe the truth. More about *cross-entropy* can be found <a href=\"http://colah.github.io/posts/2015-09-Visual-Information/\"> here </a>. To implement the *cross-entropy* function, let define first a place holder to assign the predicted probabilities, then the *cross-entropy* $-\\sum_i y_i' log(y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To train the model, tensorflow uses the <a href=\"http://colah.github.io/posts/2015-08-Backprop/\">backpropagation algorithm </a> to determine the variables that minimize the cost you put in the model. Tensorflow then gives many  <a href=\"https://www.tensorflow.org/versions/0.6.0/api_docs/python/train.html#optimizers\">optimization algorithms</a> to modify the variables and reduce the cost. For example, here *cross_entropy* cost function is monimized using the *gradient descent algorithm* with a learning rate of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once having the network structure, next step would be initializing the variable, lauch the model in a *Session* and run it with initial variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will start training the model by iterating by *N_iter* steps using *stochastic gradient descent* optimization algorithm. *Stochastic* here is in a sense that each step takes a \"batch\" data points from the training set and pass them to the session via *feed_dict* function intead of the whole training data. The batch size is 100 in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_iter = 2000\n",
    "for i in range(N_iter):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluating the model\n",
    "\n",
    "The accuracy of the trained network is evaluated as follows. Function *argmax* helps to find the correct label of each input, while *tf.equal* gives a list of boolean values to say whether the predictions match the reference label or not. Cast these boolean to 0 or 1, then taking the mean gives the accuracy of the network of the input data by *feed_dict* (this case is testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training data: 0.923436\n",
      "Accuracy for testing data 0.9194\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print \"Accuracy for training data:\", sess.run(accuracy, feed_dict={x: mnist.train.images, y_: mnist.train.labels})\n",
    "print \"Accuracy for testing data\", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multilayer Convolutional Network\n",
    "\n",
    "The above network is very simple, and model's accuracy is rather unsatisfactory. We will build a convolutional network to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "We will create the following two handy function to initialize weights or bias of a given *shape*. Weights are initialized with a small mount of noises to break the symestry and prevent from zero-gradient. It is also suggested by tensorflow that bias should be slightly positive to avoid \"dead\" neurons.\n",
    "\n",
    "Here function *tf.truncated_nomal(shape, mean = 0.0, stddev = 1.0)* returns random values follow a normal distribution of a give shape. *Truncated* here means values of magnitude more than 2 std are dropped and re-picked. *tf.constant (value, shape)* return constant *value* of given *shape*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling\n",
    "\n",
    "Define the two handy functions to convolve and pool features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Convolutional Layer\n",
    "\n",
    "The first layer consists of convolution and followed by max pooling. The convolutional will compute 32 features for each 5x5 patch, and the weight tensor has a shape of [5, 5, 1, 32] accordingly (patch size 5x5, 1 input channel, 32 output channels. We will also have a bias vector with a component for each output channel.\n",
    "\n",
    "After the first layer, an input image of size 28x28 after the convolution (zero padding) remains 28x28, max pooling by 2x2, will become an image of size 32x14x14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Convolutional Layer\n",
    "The second layer has 64 features for each 5x5 patch. There are 32 input channels from the first layers, and 64 output channels. After this layer, we will have 64 features of size 7x7 for each 28x28 input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely Connected Layer\n",
    "\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14\n",
      "step 100, training accuracy 0.84\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 1\n",
      "step 500, training accuracy 0.92\n",
      "step 600, training accuracy 0.94\n",
      "step 700, training accuracy 0.98\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.94\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 1\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 0.96\n",
      "test accuracy 0.9762\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(N_iter):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print \"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, \n",
    "                                                                y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
